{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# __Language Processing Pipeline__\n",
        "## Pipeline is basically a bunch of components which comes after tokenizer like tagger, parser, ner (Name Entity Recognition) and we can have number of processing steps."
      ],
      "metadata": {
        "id": "6pHHBaWg8cxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "Q2lzEPE2BOO_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\") # Here, we created a blank language processing pipeline\n",
        "\n",
        "doc = nlp(\"Captain America ate 100$ of Apples. Then he said he can do that all day.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDFSkMIZBRbI",
        "outputId": "0b4f7563-b473-41f4-8fa8-b7cb379f0149"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain\n",
            "America\n",
            "ate\n",
            "100\n",
            "$\n",
            "of\n",
            "Apples\n",
            ".\n",
            "Then\n",
            "he\n",
            "said\n",
            "he\n",
            "can\n",
            "do\n",
            "that\n",
            "all\n",
            "day\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s-gnVAcBnZE",
        "outputId": "cf571108-9a72-43b9-e11a-42a3996d454c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here, we can see a blank array means by default there is no components of pipelines. Now we need to use pre-trained pipeline. Refer https://spacy.io/usage/models#quickstart and download the command using __python -m spacy download en_core_web_sm__ (NB: This is for English) command."
      ],
      "metadata": {
        "id": "5dboO2F5DCcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# en = english and sm = small"
      ],
      "metadata": {
        "id": "5d16gUKAC8nz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking all the components\n",
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8-rXxt0EWM7",
        "outputId": "72682927-1daa-43c6-e5f5-431a4fa99c33"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f79afed9340>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f79afed9be0>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f79b1548c80>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f79b1b1acc0>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f79b17d3f00>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f79b1548b30>)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa7qRN1dHJGW",
        "outputId": "a3802c10-c8f0-44cf-e728-71b8ad4d6858"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Captain America ate 100$ of Apples. Then he said he can do that all day.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token, \" | \", token.pos_, \" | \", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9nkNU7kFoF_",
        "outputId": "dd12d1a9-44b5-4ddf-910b-6b36c5e1c6b2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain  |  PROPN  |  Captain\n",
            "America  |  PROPN  |  America\n",
            "ate  |  VERB  |  eat\n",
            "100  |  NUM  |  100\n",
            "$  |  NUM  |  $\n",
            "of  |  ADP  |  of\n",
            "Apples  |  PROPN  |  Apples\n",
            ".  |  PUNCT  |  .\n",
            "Then  |  ADV  |  then\n",
            "he  |  PRON  |  he\n",
            "said  |  VERB  |  say\n",
            "he  |  PRON  |  he\n",
            "can  |  AUX  |  can\n",
            "do  |  VERB  |  do\n",
            "that  |  PRON  |  that\n",
            "all  |  DET  |  all\n",
            "day  |  NOUN  |  day\n",
            ".  |  PUNCT  |  .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __pos = Parts of Speech.__\n",
        "## When we are speaking an english sentence, every word has some meaning For ex: Siddharth loves to play Football (Here, Siddharth & Football are noun and loves & play are verb; Nouns also are of 2 types PROPN (Proper Noun) and Common Noun)\n",
        "## __lemmatizer__ means it will show base word. For ex: ate --> eat, played --> play, etc."
      ],
      "metadata": {
        "id": "Y4WkZkg9GClT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking what is NER\n",
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, \" | \", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QyWozL9F8LR",
        "outputId": "6c2dabb0-db6e-455e-d0d5-c342e7216236"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  ORG\n",
            "$45 billion  |  MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So, here it is recognizing the entities as Organization and Money."
      ],
      "metadata": {
        "id": "GMsrXm9iUTBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDe2AqZBUDHw",
        "outputId": "a1ef6619-bc67-4db4-8364-ae71fb17794b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "$45 billion  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing these entities in a fancy way\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "NlIqm0upUmNQ",
        "outputId": "c2c2a75c-6110-4c43-c73a-8a7a936bdd1a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    Tesla Inc\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\\n</mark>\\n is going to acquire twitter for \\n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    $45 billion\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\\n</mark>\\n</div>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding NER component to our English Language pipeline\n",
        "source_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "nlp.add_pipe(\"ner\", source = source_nlp)\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyUM3gubVBLh",
        "outputId": "b71410f4-a200-4be3-c091-f0b95d48d83e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjk07HEaWTpN",
        "outputId": "15998243-dc7c-478a-ac8f-24f7d00d93d3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "$45 billion  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXPVmvoZWijX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}